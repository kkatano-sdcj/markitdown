"""
MarkItDown AI Service - MarkItDownã¨LLMã®çµ±åˆ
MarkitDown.mdcã®å®Ÿè£…ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ãŸã€ç”»åƒèª¬æ˜ã‚’å«ã‚€é«˜åº¦ãªå¤‰æ›å‡¦ç†
"""
import os
import logging
from typing import Optional, Dict, Any
from markitdown import MarkItDown
from openai import OpenAI
from app.models.data_models import ConversionResult, ConversionStatus
from app.services.cancel_manager import cancel_manager
import time
import uuid
import asyncio

logger = logging.getLogger(__name__)

class MarkItDownAIService:
    """MarkItDownã¨OpenAI LLMã‚’çµ±åˆã—ãŸAIå¤‰æ›ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.llm_client = None
        self.llm_model = "gpt-4o-mini"
        self.md_normal = None
        self.md_ai = None
        self._initialize_services()
    
    def _initialize_services(self):
        """ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç”¨ã®MarkItDownï¼ˆLLMãªã—ï¼‰
        self.md_normal = MarkItDown(enable_plugins=False)
        
        # AI modeç”¨ã®MarkItDownï¼ˆLLMçµ±åˆï¼‰
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            try:
                self.llm_client = OpenAI(api_key=api_key)
                # MarkItDownã«LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç›´æ¥æ¸¡ã™ï¼ˆMarkitDown.mdcã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã†ï¼‰
                self.md_ai = MarkItDown(
                    llm_client=self.llm_client,
                    llm_model=self.llm_model
                )
                logger.info(f"AI mode initialized with {self.llm_model}")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")
                self.md_ai = self.md_normal
        else:
            logger.warning("OpenAI API key not found, AI mode disabled")
            self.md_ai = self.md_normal
    
    async def convert_with_ai(
        self, 
        file_path: str, 
        output_filename: str, 
        use_ai_mode: bool = False,
        progress_callback = None
    ) -> ConversionResult:
        """
        AIçµ±åˆã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›
        
        Args:
            file_path: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
            use_ai_mode: AIå¤‰æ›ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            
        Returns:
            ConversionResult: å¤‰æ›çµæœ
        """
        conversion_id = str(uuid.uuid4())
        start_time = time.time()
        
        # Register conversion for cancellation
        cancel_manager.register_conversion(conversion_id)
        
        try:
            # Check if cancelled
            if cancel_manager.is_cancelled(conversion_id):
                raise asyncio.CancelledError("å¤‰æ›ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            
            # Send initial progress
            if progress_callback:
                await progress_callback(conversion_id, 10, "processing", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ä¸­...", os.path.basename(file_path))
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ï¼ˆMarkitDown.mdcã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …ã«å¾“ã†ï¼‰
            if not self._validate_file(file_path):
                raise ValueError("ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # Check if cancelled
            if cancel_manager.is_cancelled(conversion_id):
                raise asyncio.CancelledError("å¤‰æ›ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            
            if progress_callback:
                await progress_callback(conversion_id, 20, "processing", "å¤‰æ›æº–å‚™ä¸­...", os.path.basename(file_path))
            
            # é©åˆ‡ãªMarkItDownã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’é¸æŠ
            md = self.md_ai if use_ai_mode and self.is_ai_available() else self.md_normal
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’ç¢ºèª
            file_ext = os.path.splitext(file_path)[1].lower()[1:]
            
            # Check if cancelled
            if cancel_manager.is_cancelled(conversion_id):
                raise asyncio.CancelledError("å¤‰æ›ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            
            if progress_callback:
                await progress_callback(conversion_id, 30, "processing", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­...", os.path.basename(file_path))
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹åˆ¥å‡¦ç†
            if file_ext in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp']:
                # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¯ç›´æ¥å‡¦ç†ï¼ˆMarkItDownã¯ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ãªã„ãŸã‚ï¼‰
                ai_description = None
                if use_ai_mode and self.is_ai_available():
                    if progress_callback:
                        await progress_callback(conversion_id, 50, "processing", "AIåˆ†æä¸­...", os.path.basename(file_path))
                    # AI modeã®å ´åˆã€MarkItDownã§AIåˆ†æã‚’å–å¾—
                    result = md.convert(file_path)
                    if result and result.text_content:
                        ai_description = result.text_content
                
                if progress_callback:
                    await progress_callback(conversion_id, 70, "processing", "OCRå‡¦ç†ä¸­...", os.path.basename(file_path))
                # OCRå‡¦ç†ã‚’å«ã‚€ç”»åƒå‡¦ç†
                markdown_content = self._process_standalone_image(file_path, ai_description)
            else:
                # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®å¤‰æ›
                logger.info(f"Converting {file_path} with {'AI' if use_ai_mode else 'Normal'} mode")
                result = md.convert(file_path)
                
                if not result or not result.text_content:
                    raise ValueError("å¤‰æ›çµæœãŒç©ºã§ã™")
                
                markdown_content = result.text_content
                
                if use_ai_mode and self.is_ai_available():
                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®AIå¼·åŒ–
                    if file_ext in ['docx', 'pptx', 'pdf']:
                        markdown_content = self._enhance_document_with_ai(markdown_content, file_ext)
            
            if progress_callback:
                await progress_callback(conversion_id, 90, "processing", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ä¸­...", os.path.basename(file_path))
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            output_path = os.path.join("./converted", output_filename)
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            processing_time = time.time() - start_time
            
            if progress_callback:
                await progress_callback(conversion_id, 100, "completed", "å¤‰æ›å®Œäº†", os.path.basename(file_path))
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ­ã‚°è¨˜éŒ²ï¼ˆMarkitDown.mdcã®å“è³ªä¿è¨¼ã«å¾“ã†ï¼‰
            self._log_conversion_metrics(file_path, markdown_content, processing_time)
            
            return ConversionResult(
                id=conversion_id,
                input_file=os.path.basename(file_path),
                output_file=output_filename,
                status=ConversionStatus.COMPLETED,
                processing_time=processing_time,
                markdown_content=markdown_content
            )
            
        except asyncio.CancelledError as e:
            logger.info(f"Conversion {conversion_id} was cancelled")
            if progress_callback:
                await progress_callback(conversion_id, 0, "cancelled", "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ", os.path.basename(file_path))
            return ConversionResult(
                id=conversion_id,
                input_file=os.path.basename(file_path),
                status=ConversionStatus.FAILED,
                error_message="å¤‰æ›ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ",
                processing_time=time.time() - start_time
            )
        except Exception as e:
            logger.error(f"Conversion error: {str(e)}")
            return ConversionResult(
                id=conversion_id,
                input_file=os.path.basename(file_path),
                status=ConversionStatus.FAILED,
                error_message=f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}",
                processing_time=time.time() - start_time
            )
        finally:
            # Unregister conversion
            cancel_manager.unregister_conversion(conversion_id)
    
    def _validate_file(self, file_path: str) -> bool:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ï¼ˆMarkitDown.mdcã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …ã«å¾“ã†ï¼‰
        
        Args:
            file_path: æ¤œè¨¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            bool: æ¤œè¨¼æˆåŠŸã‹ã©ã†ã‹
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™
        max_size = 100 * 1024 * 1024  # 100MB
        file_size = os.path.getsize(file_path)
        if file_size > max_size:
            raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã¾ã™: {file_size/1024/1024:.2f}MB (æœ€å¤§: {max_size/1024/1024}MB)")
        
        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
        allowed_extensions = {
            '.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls',
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp',
            '.html', '.txt', '.csv', '.json', '.xml', '.zip',
            '.mp3', '.wav', '.ogg', '.m4a', '.flac'
        }
        ext = os.path.splitext(file_path)[1].lower()
        if ext not in allowed_extensions:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}")
        
        return True
    
    def _enhance_document_with_ai(self, markdown_content: str, file_type: str) -> str:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’AIã§å¼·åŒ–ï¼ˆç”»åƒèª¬æ˜ã‚’åˆ†é›¢ã—ã¦è¡¨è¨˜ï¼‰
        
        Args:
            markdown_content: å…ƒã®Markdownå†…å®¹
            file_type: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
            
        Returns:
            str: å¼·åŒ–ã•ã‚ŒãŸMarkdown
        """
        # ç”»åƒã®èª¬æ˜ã‚’åˆ†é›¢ã—ã¦æ•´ç†
        enhanced_content = f"# AI Enhanced Document\n\n"
        enhanced_content += f"**Document Type**: {file_type.upper()}\n"
        enhanced_content += f"**AI Processing**: Enabled (Model: {self.llm_model})\n"
        enhanced_content += f"**Features**: Image descriptions, chart analysis, enhanced formatting\n\n"
        enhanced_content += "---\n\n"
        
        # ç”»åƒèª¬æ˜ã‚’æŠ½å‡ºã—ã¦åˆ†é›¢
        enhanced_content += self._separate_image_descriptions(markdown_content)
        
        return enhanced_content
    
    def _separate_image_descriptions(self, markdown_content: str) -> str:
        """
        ç”»åƒã®èª¬æ˜ã‚’å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰åˆ†é›¢ã—ã¦æ•´ç†
        OCRã§èª­ã¿å–ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã¯å…ƒã®ä½ç½®ã«é…ç½®
        
        Args:
            markdown_content: å…ƒã®Markdownå†…å®¹
            
        Returns:
            str: æ•´ç†ã•ã‚ŒãŸMarkdown
        """
        import re
        
        lines = markdown_content.split('\n')
        result_lines = []
        image_descriptions = []
        image_counter = 0
        
        i = 0
        while i < len(lines):
            line = lines[i]
            
            # ç”»åƒãƒªãƒ³ã‚¯ã‚’æ¤œå‡º
            if line.strip().startswith('!['):
                image_counter += 1
                
                # ç”»åƒã®èª¬æ˜ã‚’æŠ½å‡º
                match = re.match(r'!\[(.*?)\]\((.*?)\)', line)
                if match:
                    description = match.group(1)
                    image_path = match.group(2)
                    
                    # å…ƒã®ä½ç½®ã«ã¯ç”»åƒå‚ç…§ã‚’é…ç½®
                    image_ref = f"![Image {image_counter}]({image_path})"
                    result_lines.append(image_ref)
                    
                    # AIç”Ÿæˆã®èª¬æ˜ã‹ã‚‰ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                    extracted_text = self._extract_ocr_text(description)
                    
                    if extracted_text:
                        # OCRã§èª­ã¿å–ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã®ä½ç½®ã«è¿½åŠ 
                        result_lines.append("")
                        result_lines.append("**ğŸ“ ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆ:**")
                        for text in extracted_text:
                            result_lines.append(f"- {text}")
                        result_lines.append("")
                    
                    # èª¬æ˜ãŒé•·ã„å ´åˆï¼ˆAIç”Ÿæˆã®èª¬æ˜ï¼‰ã€åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    if len(description) > 50:  # AIç”Ÿæˆã®èª¬æ˜ã¯é€šå¸¸é•·ã„
                        image_descriptions.append({
                            'number': image_counter,
                            'description': description,
                            'path': image_path,
                            'extracted_text': extracted_text
                        })
                    else:
                        # çŸ­ã„èª¬æ˜ã¯ãã®ã¾ã¾æ®‹ã™
                        if not extracted_text:  # OCRãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã®ã¿
                            result_lines.append(f"*{description}*")
                else:
                    result_lines.append(line)
            else:
                result_lines.append(line)
            
            i += 1
        
        # å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        final_content = "## ğŸ“„ Document Content\n\n"
        final_content += '\n'.join(result_lines)
        
        # ç”»åƒèª¬æ˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        if image_descriptions:
            final_content += "\n\n---\n\n"
            final_content += "## ğŸ–¼ï¸ AI Image Analysis\n\n"
            
            for img_desc in image_descriptions:
                final_content += f"### Image {img_desc['number']}\n\n"
                final_content += f"**File**: `{img_desc['path']}`\n\n"
                
                # OCRãƒ†ã‚­ã‚¹ãƒˆã¯æ—¢ã«å…ƒã®ä½ç½®ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ã“ã“ã§ã¯åˆ†æã®ã¿
                final_content += "**AI Analysis**:\n"
                # OCRãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’é™¤ã„ãŸåˆ†æã‚’è¡¨ç¤º
                analysis = self._remove_ocr_from_description(img_desc['description'], img_desc['extracted_text'])
                final_content += f"{analysis}\n\n"
        
        return final_content
    
    def _extract_ocr_text(self, description: str) -> list:
        """
        AIèª¬æ˜æ–‡ã‹ã‚‰ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆOCRçµæœï¼‰ã‚’æŠ½å‡º
        å…ƒã®è¨€èªã®ã¾ã¾æŠ½å‡ºï¼ˆæ—¥æœ¬èªã¯æ—¥æœ¬èªã€è‹±èªã¯è‹±èªã®ã¾ã¾ï¼‰
        
        Args:
            description: AIç”Ÿæˆã®ç”»åƒèª¬æ˜
            
        Returns:
            list: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        extracted_texts = []
        
        # ã‚ˆã‚Šå¹…åºƒã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã€å¼•ç”¨ç¬¦å†…ã®ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        # è¨€èªã‚’å•ã‚ãšã€å¼•ç”¨ç¬¦ã§å›²ã¾ã‚ŒãŸã‚‚ã®ã¯ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã¨åˆ¤æ–­
        quote_patterns = [
            r'"([^"]+)"',  # ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ
            r"'([^']+)'",  # ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ  
            r'ã€Œ([^ã€]+)ã€',  # æ—¥æœ¬èªã®é‰¤æ‹¬å¼§
            r'ã€([^ã€]+)ã€',  # æ—¥æœ¬èªã®äºŒé‡é‰¤æ‹¬å¼§
            r'"([^"]+)"',  # å…¨è§’ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ
            r"'([^']+)'",  # å…¨è§’ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ
            r'ã€([^ã€‘]+)ã€‘',  # éš…ä»˜ãæ‹¬å¼§
            r'ï¼»([^ï¼½]+)ï¼½',  # å…¨è§’è§’æ‹¬å¼§
            r'\[([^\]]+)\]',  # åŠè§’è§’æ‹¬å¼§ï¼ˆUIãƒ©ãƒ™ãƒ«ç­‰ï¼‰
            r'ã€ˆ([^ã€‰]+)ã€‰',  # å±±æ‹¬å¼§
            r'ã€Š([^ã€‹]+)ã€‹',  # äºŒé‡å±±æ‹¬å¼§
        ]
        
        # ã™ã¹ã¦ã®å¼•ç”¨ç¬¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        for pattern in quote_patterns:
            matches = re.findall(pattern, description)
            for match in matches:
                # é‡è¤‡ã‚’é¿ã‘ã€çŸ­ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
                if match not in extracted_texts and len(match.strip()) >= 1:
                    # èª¬æ˜çš„ãªè‹±èªãƒ•ãƒ¬ãƒ¼ã‚ºã¯é™¤å¤–ï¼ˆå®Ÿéš›ã®ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆã§ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
                    exclude_phrases = [
                        'the image', 'this shows', 'appears to be', 'seems to be',
                        'can be seen', 'is visible', 'is displayed', 'is shown',
                        'the user', 'the system', 'the interface', 'the screen',
                        'contains', 'includes', 'features', 'with text'
                    ]
                    
                    # é™¤å¤–ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å«ã¾ãªã„å ´åˆã®ã¿è¿½åŠ 
                    if not any(phrase in match.lower() for phrase in exclude_phrases):
                        extracted_texts.append(match)
        
        # ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ç¶šããƒ†ã‚­ã‚¹ãƒˆã‚‚æŠ½å‡ºï¼ˆå¼•ç”¨ç¬¦ãªã—ã®å ´åˆï¼‰
        # ã“ã‚Œã‚‰ã¯å®Ÿéš›ã®UIè¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
        ui_patterns = [
            # è‹±èªãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(?:titled|labeled|marked as|reading|saying|displaying|showing|written as)\s+([^\.,;:"\']+)',
            r'(?:button|field|label|text|title|heading|menu|tab|option|caption)\s+(?:reads?|says?|shows?|displays?|contains?)\s+([^\.,;:"\']+)',
            r'(?:with the text|with text|text reading|displaying text)\s+([^\.,;:"\']+)',
            
            # æ—¥æœ¬èªãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(?:ãƒœã‚¿ãƒ³|ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰|ãƒ©ãƒ™ãƒ«|ãƒ†ã‚­ã‚¹ãƒˆ|ã‚¿ã‚¤ãƒˆãƒ«|è¦‹å‡ºã—|ãƒ¡ãƒ‹ãƒ¥ãƒ¼|ã‚¿ãƒ–|ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³)(?:ã«|ã®|ãŒ|ã¯|ã¨ã—ã¦)\s*([^\ã€‚ã€ï¼›ï¼š""'']+)',
            r'(?:ã¨æ›¸ã‹ã‚Œ|ã¨è¡¨ç¤ºã•ã‚Œ|ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆ|ã¨ã„ã†æ–‡å­—|ã¨ã„ã†è¡¨è¨˜)(?:ã¦ã„ã‚‹|ãŸ)\s*([^\ã€‚ã€ï¼›ï¼š""'']+)',
            r'(?:ãƒ†ã‚­ã‚¹ãƒˆ|æ–‡å­—|è¡¨è¨˜|æ–‡è¨€)(?:ã¯|ãŒ)\s*([^\ã€‚ã€ï¼›ï¼š""'']+)(?:ã§ã™|ã§ã‚ã‚‹|ã¨ãªã£)',
        ]
        
        for pattern in ui_patterns:
            matches = re.findall(pattern, description, re.IGNORECASE | re.UNICODE)
            for match in matches:
                cleaned = match.strip()
                # é•·ã™ãã‚‹ã‚‚ã®ã¯èª¬æ˜æ–‡ã®å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§é™¤å¤–
                if cleaned not in extracted_texts and 1 <= len(cleaned) <= 100:
                    # å‹•è©ã‚„å† è©ã ã‘ã®ã‚‚ã®ã¯é™¤å¤–
                    if not cleaned.lower() in ['is', 'are', 'was', 'were', 'the', 'a', 'an', 'ã§ã™', 'ã¾ã™', 'ã§ã‚ã‚‹']:
                        extracted_texts.append(cleaned)
        
        # æ—¥æœ¬èªã®æ–‡è„ˆã‹ã‚‰æŠ½å‡ºï¼ˆã‚ˆã‚Šè‡ªç„¶ãªæ—¥æœ¬èªè¡¨ç¾ï¼‰
        japanese_context_patterns = [
            r'([ã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾¯]+)(?:ã¨ã„ã†ãƒœã‚¿ãƒ³|ã¨ã„ã†ãƒªãƒ³ã‚¯|ã¨ã„ã†ã‚¿ãƒ–|ã¨ã„ã†ãƒ¡ãƒ‹ãƒ¥ãƒ¼)',
            r'([ã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾¯]+)(?:ã¨è¡¨ç¤º|ã¨æ›¸ã‹ã‚Œ|ã¨è¨˜è¼‰)',
            r'ç”»é¢ã«(?:ã¯)?([ã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾¯]+)(?:ã¨|ãŒ)',
        ]
        
        for pattern in japanese_context_patterns:
            matches = re.findall(pattern, description)
            for match in matches:
                if match not in extracted_texts and len(match.strip()) >= 1:
                    extracted_texts.append(match)
        
        # ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤ï¼‰
        extracted_texts = [text.strip() for text in extracted_texts if text.strip()]
        
        # å…ƒã®é †åºã‚’ä¿æŒã—ã¤ã¤é‡è¤‡ã‚’å‰Šé™¤
        seen = set()
        unique_texts = []
        for text in extracted_texts:
            if text not in seen:
                seen.add(text)
                unique_texts.append(text)
        
        return unique_texts
    
    def _remove_ocr_from_description(self, description: str, extracted_texts: list) -> str:
        """
        èª¬æ˜æ–‡ã‹ã‚‰OCRãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦AIåˆ†æã®ã¿ã‚’æ®‹ã™
        
        Args:
            description: å…ƒã®èª¬æ˜æ–‡
            extracted_texts: æŠ½å‡ºã•ã‚ŒãŸOCRãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            str: OCRãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤ã„ãŸåˆ†æ
        """
        import re
        
        result = description
        
        # OCRãƒ†ã‚­ã‚¹ãƒˆã«é–¢é€£ã™ã‚‹è¨˜è¿°ã‚’ç°¡ç•¥åŒ–
        if extracted_texts:
            # å„æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆã«é–¢ã™ã‚‹è¨˜è¿°ã‚’å‰Šé™¤ã¾ãŸã¯ç°¡ç•¥åŒ–
            for text in extracted_texts:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€æ–‡ã‚’è¦‹ã¤ã‘ã¦ç°¡ç•¥åŒ–
                result = re.sub(
                    rf'[^.]*["\']?{re.escape(text)}["\']?[^.]*\.',
                    '',
                    result,
                    flags=re.IGNORECASE
                )
        
        # æ®‹ã£ãŸèª¬æ˜ã‚’æ•´ç†
        sentences = result.split('.')
        cleaned_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:
                # OCRé–¢é€£ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å«ã‚€æ–‡ã¯é™¤å¤–
                ocr_phrases = ['titled', 'labeled', 'text reads', 'displays text', 'shows text', 'contains text']
                if not any(phrase in sentence.lower() for phrase in ocr_phrases):
                    cleaned_sentences.append(sentence)
        
        return '. '.join(cleaned_sentences) + '.' if cleaned_sentences else description
    
    def _process_standalone_image(self, file_path: str, ai_description: str = None) -> str:
        """
        ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆç­‰ï¼‰ã‚’å‡¦ç†
        OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€AIåˆ†æã¨çµ„ã¿åˆã‚ã›ã‚‹
        
        Args:
            file_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            ai_description: AIç”Ÿæˆã®èª¬æ˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸMarkdown
        """
        import re
        from services.paddle_ocr_service import PaddleOCRService
        
        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path) / 1024  # KB
        
        # Markdownã®æ§‹é€ ã‚’ä½œæˆ
        formatted_content = f"# Image Analysis: {file_name}\n\n"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        formatted_content += f"## ğŸ“ File Information\n\n"
        formatted_content += f"- **File**: `{file_name}`\n"
        formatted_content += f"- **Size**: {file_size:.2f} KB\n"
        formatted_content += f"- **Type**: {os.path.splitext(file_name)[1].upper()[1:]} Image\n"
        if ai_description:
            formatted_content += f"- **AI Model**: {self.llm_model}\n"
        formatted_content += "\n---\n\n"
        
        # OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§ã‚‚å®Ÿè¡Œï¼‰
        ocr_service = PaddleOCRService()
        ocr_text = ""
        
        if ocr_service.is_available():
            try:
                # OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                ocr_result = ocr_service.extract_text(file_path)
                if ocr_result and "OCR extraction failed" not in ocr_result and "No text detected" not in ocr_result:
                    ocr_text = ocr_result
            except Exception as e:
                logger.warning(f"OCR extraction failed: {e}")
        
        # AIèª¬æ˜ã‹ã‚‰å¼•ç”¨ç¬¦å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚æŠ½å‡º
        extracted_texts = self._extract_ocr_text(ai_description) if ai_description else []
        
        # OCRãƒ†ã‚­ã‚¹ãƒˆã¨AIæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆ
        all_texts = []
        
        # OCRãƒ†ã‚­ã‚¹ãƒˆã‚’è¡Œã”ã¨ã«åˆ†å‰²
        if ocr_text:
            ocr_lines = [line.strip() for line in ocr_text.split('\n') if line.strip()]
            # [low confidence] ã‚„ [unverified] ã‚’é™¤å»
            for line in ocr_lines:
                cleaned = re.sub(r'\s*\[(?:low confidence|unverified).*?\]', '', line).strip()
                if cleaned and cleaned not in all_texts:
                    all_texts.append(cleaned)
        
        # AIæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
        for text in extracted_texts:
            if text not in all_texts:
                all_texts.append(text)
        
        # ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if all_texts:
            formatted_content += f"## ğŸ“ Extracted Text\n\n"
            formatted_content += "**Text found in the image:**\n\n"
            for text in all_texts:
                formatted_content += f"- {text}\n"
            formatted_content += "\n---\n\n"
        
        # ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        formatted_content += f"## ğŸ–¼ï¸ Image Preview\n\n"
        formatted_content += f"![{file_name}]({file_path})\n\n"
        
        formatted_content += "---\n\n"
        
        # AIåˆ†æï¼ˆãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’é™¤ã„ãŸç´”ç²‹ãªåˆ†æï¼‰
        if ai_description:
            formatted_content += "## ğŸ¤– AI Analysis\n\n"
            # OCRãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤ã„ãŸåˆ†æã‚’è¡¨ç¤º
            analysis = self._remove_ocr_from_description(ai_description, all_texts)
            if analysis:
                formatted_content += f"{analysis}\n\n"
            else:
                formatted_content += "This image contains the text shown above. "
                formatted_content += "The layout and visual elements suggest this is a screenshot or document scan.\n\n"
        elif not all_texts:
            # OCRãƒ†ã‚­ã‚¹ãƒˆã‚‚AIåˆ†æã‚‚ãªã„å ´åˆ
            formatted_content += "## ğŸ“ Notes\n\n"
            formatted_content += "No text was detected in this image.\n\n"
        
        return formatted_content
    
    def _format_image_result(self, markdown_content: str, file_path: str) -> str:
        """
        ç”»åƒå¤‰æ›çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆç”»åƒå˜ä½“ã®å ´åˆï¼‰
        
        Args:
            markdown_content: å¤‰æ›ã•ã‚ŒãŸMarkdownå†…å®¹
            file_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸMarkdown
        """
        import re
        
        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path) / 1024  # KB
        
        formatted_content = f"# Image Analysis: {file_name}\n\n"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        formatted_content += f"## ğŸ“ File Information\n\n"
        formatted_content += f"- **File**: `{file_name}`\n"
        formatted_content += f"- **Size**: {file_size:.2f} KB\n"
        formatted_content += f"- **AI Model**: {self.llm_model}\n\n"
        
        formatted_content += "---\n\n"
        
        # ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        formatted_content += f"## ğŸ–¼ï¸ Image Preview\n\n"
        formatted_content += f"![{file_name}]({file_path})\n\n"
        
        formatted_content += "---\n\n"
        
        # AIåˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³
        formatted_content += "## ğŸ¤– AI Analysis\n\n"
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ç”»åƒèª¬æ˜ã‚’æŠ½å‡º
        # MarkItDownã¯ç”»åƒã®å ´åˆã€èª¬æ˜æ–‡ã®ã¿ã‚’è¿”ã™ã“ã¨ãŒå¤šã„
        if markdown_content.strip().startswith('!['):
            # ç”»åƒãƒªãƒ³ã‚¯ã‹ã‚‰èª¬æ˜ã‚’æŠ½å‡º
            match = re.match(r'!\[(.*?)\]\((.*?)\)', markdown_content)
            if match:
                description = match.group(1)
                formatted_content += f"{description}\n\n"
            else:
                formatted_content += markdown_content
        else:
            # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜
            formatted_content += markdown_content
        
        return formatted_content
    
    def _log_conversion_metrics(self, file_path: str, result: str, processing_time: float):
        """
        å¤‰æ›ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ­ã‚°è¨˜éŒ²ï¼ˆMarkitDown.mdcã®å“è³ªä¿è¨¼ã«å¾“ã†ï¼‰
        
        Args:
            file_path: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            result: å¤‰æ›çµæœ
            processing_time: å‡¦ç†æ™‚é–“
        """
        file_size = os.path.getsize(file_path)
        output_size = len(result.encode('utf-8'))
        
        logger.info(f"å¤‰æ›å®Œäº†: {os.path.basename(file_path)}")
        logger.info(f"å…¥åŠ›ã‚µã‚¤ã‚º: {file_size/1024:.2f}KB")
        logger.info(f"å‡ºåŠ›ã‚µã‚¤ã‚º: {output_size/1024:.2f}KB")
        logger.info(f"å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        logger.info(f"å¤‰æ›åŠ¹ç‡: {output_size/file_size:.2f}")
    
    def is_ai_available(self) -> bool:
        """AIæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª"""
        return self.llm_client is not None and self.md_ai is not None
    
    def get_status(self) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            'service': 'MarkItDown AI Service',
            'ai_available': self.is_ai_available(),
            'llm_model': self.llm_model if self.is_ai_available() else None,
            'markitdown_version': '0.1.2',
            'features': {
                'image_description': self.is_ai_available(),
                'chart_analysis': self.is_ai_available(),
                'document_enhancement': self.is_ai_available(),
                'ocr': True,
                'metadata_extraction': True
            }
        }
    
    async def batch_convert_with_ai(
        self, 
        file_paths: list[str], 
        use_ai_mode: bool = False
    ) -> list[ConversionResult]:
        """
        ãƒãƒƒãƒå¤‰æ›ï¼ˆMarkitDown.mdcã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã«å¾“ã†ï¼‰
        
        Args:
            file_paths: å¤‰æ›ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            use_ai_mode: AIå¤‰æ›ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            
        Returns:
            list[ConversionResult]: å¤‰æ›çµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []
        
        # MarkItDownã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†åˆ©ç”¨ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
        md = self.md_ai if use_ai_mode and self.is_ai_available() else self.md_normal
        
        for file_path in file_paths:
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            output_filename = f"{base_name}.md"
            
            result = await self.convert_with_ai(file_path, output_filename, use_ai_mode)
            results.append(result)
        
        return results